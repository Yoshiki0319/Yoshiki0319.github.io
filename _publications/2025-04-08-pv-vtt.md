---
title: "PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation"
collection: publications
category: conferences
permalink: /publication/pv-vtt
date: 2025-04-08
venue: "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025"
excerpt: "To address this limitation, we present PV-VTT (f_rivacyY._iolation Y._ideo T_Iext), a unique mul-timodal dataset aimed at identifying privacy violations."
paperurl: "https://openaccess.thecvf.com/content/WACV2025/papers/Masukawa_PV-VTT_A_Privacy-Centric_Dataset_for_Mission-Specific_Anomaly_Detection_and_Natural_WACV_2025_paper.pdf"
citation: "R. Masuakwa, S. Yun, Y. Yamaguchi and M. Imani, \"PV-VTT: A Privacy-Centric Dataset for Mission-Specific Anomaly Detection and Natural Language Interpretation,\" 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Tucson, AZ, USA, 2025, pp. 6415-6424, doi: 10.1109/WACV61041.2025.00625."
---

Video crime detection is a significant application of computer vision and artificial intelligence. However, existing datasets primarily focus on detecting severe crimes by analyzing entire video clips, often neglecting the precursor ac-tivities (i.e., privacy violations) that could potentially pre-vent these crimes. To address this limitation, we present PV-VTT (f_rivacyY._iolation Y._ideo T_Iext), a unique mul-timodal dataset aimed at identifying privacy violations. PV-VTT provides detailed annotations for both video and text in scenarios. To ensure the privacy of individuals in the videos, we only provide video feature vectors, avoiding the release of any raw video data. This privacy-focused approach allows researchers to use the dataset while protecting participant confidentiality. Recognizing that privacy violations are often ambiguous and context-dependent, we propose a Graph Neural Network (GNN)-based video de-scription model. Our model generates a GNN-based prompt with image for Large Language Model (LLM), which deliver cost-effective and high-quality video descriptions. By leveraging a single video frame along with relevant text, our method reduces the number of input tokens required, maintaining descriptive quality while optimizing LLM API-usage. Extensive experiments validate the effectiveness and interpretability of our approach in video description tasks and flexibility of our PV-VTT dataset. Dataset: https://ryozomasukawa.github.io/PV-VTT.github.io/
